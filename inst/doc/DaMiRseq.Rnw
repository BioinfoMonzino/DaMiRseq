%\VignetteIndexEntry{Data Mining for RNA-seq data: normalization, features selection and classification - DaMiRseq package}
%\VignettePackage{DaMiRseq}
%\VignetteEngine{knitr::knitr}

% To compile this document
% library(tools)
% library(BiocStyle)
% library(devtools)
% library(knitr)
% setwd("./vignettes/")

% unlink(c("cache","figure","*.bst","*.sty","*.R","*.tex","*.log","*.aux","*.out","*.pdf","*.toc","*.blg","*.bbl"),recursive = T); Rcmd("Sweave --engine=knitr::knitr --pdf DaMiRseq.Rnw")
% devtools::build_vignettes()
% tools::compactPDF("../inst/doc/DaMiRseq.pdf",gs_quality = "ebook")

\documentclass{article}
<<style-knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex2(relative.path = TRUE)
@
\usepackage{subfig}% for combining multiple plots in one figure
\usepackage[section]{placeins}
\usepackage{amsmath}

\newcommand{\damir}{\textit{DaMiRseq}}

<<knitr, echo=FALSE, results="hide">>=
library("knitr")
opts_chunk$set(
  tidy=FALSE,
  dev="png",
  fig.show="hide",
#  fig.width=4, fig.height=4.5,
  fig.width=10, fig.height=8,
  fig.pos="tbh",
  cache=TRUE,
  message=FALSE)
@


\author{Mattia Chiesa}
\author{Luca Piacentini}


\affil{Immunonlogy and Functional Genomics Unit, Centro Cardiologico Monzino, IRCCS, Milan, Italy;}

\title{The DaMiRseq package - Data Mining for RNA-seq data: normalization, feature selection and classification}

\begin{document}

\maketitle

\begin{abstract}
 RNA-Seq is increasingly the method of choice for researchers studying the transcriptome. The strategies to analyze such complex high-dimensional data rely on data mining and statistical learning techniques. The \damir{} package offers a tidy pipeline that includes data mining procedures for data handling that lead up to the implementation of prediction learning methods to build classification models. The package accepts any kind of data presented as a table of raw counts and allows the inclusion of covariates that occur with the experimental setting. A series of functions enable the user to clean up the data by filtering genomic features and samples, to adjust data by identifying and removing the unwanted source of variation (i.e. batches and confounding factors) and to select the best predictors for modeling. Finally, a ``Stacking'' ensemble learning technique is applied to build a robust classification model. Every step includes a checkpoint that the user may exploit to assess the effects of data management by looking at diagnostic plots, such as clustering and heatmaps, RLE boxplots, MDS or correlation plots.
\end{abstract}

\packageVersion{\Sexpr{BiocStyle::pkg_ver("DaMiRseq")}}

\newpage

\tableofcontents

\newpage

\section{Introduction} \label{intro}
RNA-Seq is a powerful high-throughput assay that uses next-generation sequencing (NGS) technologies to discover, profile and quantify RNAs. The whole collection of RNAs defines the transcriptome, whose plasticity, allows the researcher to capture important biological information since it is sensitive to any changes occurring in response to environmental challenges, to a different healthy/disease state or to a specific genetic/epigenetic context.
The high-dimensional nature of the NGS technique makes the analysis of RNA-Seq data a demanding task that the researcher may tackle by using data mining and statistical learning procedures. Data mining usually exploits iterative and interactive processes that include selecting, preprocessing and transforming data so that only relevant features may be efficiently used by learning methods to build classification models. \\ Many software packages have been developed to assess differential expression of genomic features (i.e. genes, transcripts, exons etc.) of RNA-seq data.  (see \href{https://www.bioconductor.org/packages/release/BiocViews.html#___RNASeq}{Bioconductor\_RNASeq-packages}). Here, we propose the \damir{} package that offers a systematic and organized analysis workflow to face classification problems. \\ Briefly, we  summarize the \textbf{philosophy of \textit{DaMiRseq}} as follows. The package can be used with any technology that produces read counts of genomic features. The pipeline has been thought to direct the user, through a step-by-step data evaluation, to properly select the best strategy for each specific classification setting. It is structured into three main parts: (1) \textit{normalization}, (2) \textit{feature selection}, and (3) \textit{classification}.

The normalization step integrates conventional preprocessing and normalization procedures with data adjustment based on the estimation of the effect of ``unwanted variation''. Several factors of interest such as environments, phenotypes, demographic or clinical outcomes may influence the expression of the genomic features. However, an additional unknown source of variation may also affect the expression of any particular genomic feature and lead to confounding results and inaccurate data interpretation. The estimation of these unmeasured factors, also known as surrogate variables (sv), is crucial to fine-tune expression data in order to gain accurate prediction models \cite{leek2007capturing, jaffe2015practical}.

RNA-Seq usually consists of many features that are either irrelevant or redundant for classification purposes. Once an expression matrix of \textit{n} features \textit{x m} observations is normalized and corrected for confounding factors, the pipeline provides methods to help the user to reduce and select a subset of \textit{n} that will be subsequently used to build the prediction models. This approach, which exploits the so-called ``Feature Selection'' techniques, presents other clear advantages since: it (1) limits overfitting, (2) improves classification performance of predictors, (3) reduces time training processing, and (4) allows the production of more cost-effective models \cite{guyon2003introduction, saeys2007review}.

The reduced expression matrix, consisting of the most informative variables with respect to class, is than used to draw a ``meta-learner'' by combining the outputs of 6 different classifiers: Random Forest (RF), Na\"{i}ve Bayes (NB), 3-Nearest Neighbours (3kNN), Logistic Regression (LR), Linear Discriminant Analysis (LDA) and Support Vectors Machines (SVM); this method belongs to the ``Stack Generalization'' or, simply, ``Stacking'' ensemble learning technique \cite{friedman2001elements}. The idea behind this method is that ``weaker'' classifiers may have different generalization performances, leading to future misclassifications; by contrast, combining and weighting the prediction of several classifiers may reduce the risk of classification errors \cite{polikar2006ensemble, wolpert1992stacked}. Moreover, the weighted voting, used to assess the goodness of each weak classifiers, allows meta-learner to reach consistently high classification accuracies, better than or comparable with best weak classifiers \cite{rokach2010ensemble}.

\section{Workflow} \label{sect2}

\subsection{Input data preparation} \label{data_prep}
\damir{} package needs two tab-delimited files:

  \begin{itemize}
  \item{\textbf{Raw counts Data} - This file has to be in the classical form of a \textit{n} x \textit{m} expression table
  coming from a RNA-Seq experiment: each row represents a genomic feature (\textit{n}) while each column represents a sample (\textit{m}).
  The expression values must be un-normalized raw read counts, since \damir{} implements
  normalization and trasformation procedures that work on raw counts; the \href{http://www.bioconductor.org/help/workflows/rnaseqGene/}{RNA-seq workflow} in Bioconductor describes several techniques for preparing count matrices.
  Unique identifiers are needed for both genomic features and samples.
  }
  \item{\textbf{Class and Covariates Information} - This file contains the information related to classes/conditions (mandatory) and
  to known covariates (optional), such as demographic or clinical data, biological conditions and any sequencing or technical details. \textbf{The column containing the class/condition information
  must be labelled 'class'}. In this table, each row represents a sample and each column represents a variable (class/condition and covariates).
  The number of rows must correspond to the number of columns in 'Raw Counts Data' table, as well as the identifiers.
  }
  \end{itemize}

In this vignette we will describe the \damir{} pipeline, using as sample data a subset of Genotype-Tissue Expression (\href{http://www.gtexportal.org/static/datasets/gtex_analysis_v6/rna_seq_data/GTEx_Analysis_v6_RNA-seq_RNA-SeQCv1.1.8_gene_reads.gct.gz}{GTEx}) RNA-Seq database (dbGap Study Accession: phs000424.v6.p1) \cite{gtex2015genotype}. Briefly, GTEx project included 544 \textit{postmortem} donors and the
mRNA sequencing of 53 tissues, using 76 bp paired-end technique on Illumina HiSeq 2000: overall, 8555 samples were analyzed.
Here, we extracted data and some additional sample information (i.e. sex, age, collection center and death classification based on the Hardy scale)
for two similar brain subregions: Anterior cingulate Cortex (Bromann Area 24) and Frontal Cortex (Brodmann Area 9). These areas are close to each other and are deemed to be involved in decision making as well as in learning. This dataset is composed of 192 samples: 84 Anterior
Cingulate Cortex (ACC) and 108 Frontal Cortex (FC) samples for 56318 genes. \\ We, also, provided a data frame with classes and  covariates included.

\subsection{Import Data}
\damir{} package uses data extracted from \Biocpkg{SummarizedExperiment} class object. This class is usually employed to store either expression data produced by sequencing and any other information occuring with the experimental setting. The \Robject{SummarizedExperiment} object may be considered a matrix-like holder where rows and colums represent, respectively, features and samples. If data are not stored in a \Robject{SummarizedExperiment} object, the \Rfunction{DaMiRseq.makeSE} function helps the user to build a \Robject{SummarizedExperiment} object starting from expression and covariate data table. The function tests if expression data are in the form of raw counts, i.e. positive numbers, if 'class' variable is included in the data frame and if ``NAs'' are present in either the counts and the covariates table.
In this case \damir{} package needs two files as input data: 1) a raw counts table and 2) a class and (if present) covariates information table. In this vignette,
we will use the dataset described in Section~\ref{data_prep}; we can import the count and covariate files into R environment as follows:

<<chu_1>>=
library(DaMiRseq)
## only for example:
# rawdata.path <- system.file(package = "DaMiRseq","extdata")
# setwd(rawdata.path)
# filecounts <- list.files(rawdata.path, full.names = TRUE)[1]
# filecovariates <- list.files(rawdata.path, full.names = TRUE)[2]
# count_data <- read.delim(filecounts)
# covariate_data <- read.delim(filecovariates)
# SE<-DaMiR.makeSE(count_data, covariate_data)

@

Here, we load by the \Rfunction{data()} function the prefiltered expression data made of 21363 genes and 40 samples (20 ACC and 20 FC):

<<chu_2>>=
set.seed(12)
data(SE)
assay(SE)[1:5, c(1:5, 21:25)]
colData(SE)
@
Data are stored in the \Robject{SE} object of class \Rclass{SummarizedExperiment}. Expression data may be retrieved by the \Rfunction{assay()} accessor function and covariates information may be accessed by \Rfunction{colData()} function \footnote{See \Biocpkg{SummarizedExperiment} \cite{mm}, for more details.}.
The \textit{``colData(SE)''} data frame, containing the covariates information, includes also the \textbf{ \textit{'class'}} column as it is required by the instructions (mandatory). \\
\textbf{Notes.} We have already used the \Rfunction{set.seed()} function that allows the user to make the results of the whole pipeline reproducible.


\subsection{Preprocessing and Normalization} \label{filt_norm}
After importing the counts data, we ought to filter out non expressed and/or highly variant genes and, then, perform a normalization. Furthermore,
we can also decide to exclude from our dataset samples that show a low correlation among biological replicates and, thus, may be suspected to bear some technical artifact. The \Rfunction{DaMiR.normalization} function helps to solve the first issues, while \Rfunction{DaMiR.sampleFilt}
allows the removal of inconsistent samples.

  \subsubsection{Filtering by Expression} \label{filt_exp}
  Users can remove genes, setting the minimum number of read counts permitted across samples:
  <<chu_4>>=
  data_norm <- DaMiR.normalization(SE, minCounts=10, fSample=0.7,
                                   hyper = "no")

  @
  In this case, 19066 genes with read counts greater than 10 (\Rcode{minCounts = 10}) in at least 70\% of samples (\Rcode{fSample = 0.7}), have been selected, while 2297 have been filtered out. The dataset, consisting now of 19066 genes, is then normalized by the \Rfunction{varianceStabilizingTransformation} function
  of the \Biocpkg{DESeq2} package \cite{love2014moderated}. Using \Rfunction{assay()} function, we can see that ``VST'' transformation produces data on the log2 scale normalized with respect to the library size.

  \subsubsection{Filtering By Coefficient of Variation (CV)} \label{filt_cv}
  We named ``hypervariants'' those genes that present anomalous read counts, by comparing to the mean value across the samples. We identify them by calculating two distinct CV on sample sets that belong, respectively, to the first and the second 'class'. Genes with both 'class' CV greater than \Rcode{th.cv} will be discarded.
\textbf{Note.} Computing a 'class' restricted CV may prevent the removal of features that may be specifically associated with a certain class. This could be important in some biological contexts, for example, for immune genes whose expression under definite conditions may unveil peculiar class-gene associations. \\
  This time, we run again the \Rfunction{DaMiR.normalization} function by enabling the ``hypervariant'' gene detection by setting \Rcode{hyper = "yes"} and \Rcode{th.cv=3} (default):
  <<chu_5>>=
  data_norm <- DaMiR.normalization(SE, minCounts=10, fSample=0.7,
                                   hyper = "yes", th.cv=3)
  print(data_norm)
  assay(data_norm)[c(1:5), c(1:5, 21:25)]
  @
  The \Rcode{th.cv = 3} allows the removal of a further 14 ``hypervariant'' genes from the gene expression data matrix. The number of genes is now reduced to 19052.

  \subsubsection{Normalization} \label{normal_sec}
  After filtering, a normalization step is performed; two normalization methods are embedded in \damir{}: the \textit{Variance Stabilizing Transformation} (VST) and the
  \textit{Regularized Log Transformation} (rlog). As described in the \Biocpkg{DESeq2} vignette, VST and rlog have similar effects on data but the VST is faster than rlog,
  expecially when the number of samples increases; for these reasons, \Rfunction{varianceStabilizingTransformation} is the default normalization method, while
  \Rfunction{rlog} can be, alternatively, chosen by user.
  <<chu_6, eval=FALSE>>=
  # Time Difference, using VST or rlog for normalization:
  #
  #data_norm <- DaMiR.normalization(dds, minCounts=10, fSample=0.7, th.cv=3)
  # VST: about 80 seconds
  #
  #data_norm <- DaMiR.normalization(dds, minCounts=10, fSample=0.7, th.cv=3,
  #                                 type="rlog")
  # rlog: about 8890 seconds (i.e. 2 hours and 28 minutes!)
  @
  In this example, we run \Rfunction{DaMiR.normalization} function twice, just modifying \Rcode{type} arguments in order to test the processing time; with \Rcode{type = "vst"} (default - the same parameters used in Section~\ref{filt_cv} ) \Rfunction{DaMiR.normalization} needed 80 seconds to complete filtering and normalization, while with \Rcode{type = "rlog"} required more than 2 hours.

  \subsubsection{Sample Filtering} \label{samp_filt}
  This step introduces a sample quality checkpoint. Global gene expression should, in fact, exhibit a high correlation among biological replicates; conversely, low correlated
  samples may be suspected to bear some technical artifacts (e.g. poor RNA quality or library preparation), despite pass sequencing quality controls. If not assessed,
  these samples may, thus, negatively affect all downstream analysis. \Rfunction{DaMiR.sampleFilt} looks at the mean absolute correlation of each sample and removes those samples with a correlation
  lower than the value set in \Rcode{th.corr} argument. This threshold may be specific for different experimental settings but should be as high as possible.
  <<chu_7>>=
  data_filt <- DaMiR.sampleFilt(data_norm, th.corr=0.9)
  dim(data_filt)
  @
In this casa, zero samples were discarded because their mean absolute correlation is higher than 0.9. Data were stored in a \Robject{SummarizedExperiment} object, which contains a normalized and filtered expression \Rclass{matrix} and an updated \Robject{DataFrame} with the covariates of interest.


\subsection{Adjusting Data} \label{adj_data}
After data normalization, we propose to test for the presence of surrogate variables (sv) in order to remove the effect of putative confounding factors from the expression data. Nonetheless, the algorithm cannot distinguish among real technical batches and important biological effects (such as environmental, genetic or demographic variables) whose correction is not desirable. Therefore we enable the user to evaluate whether any of the retrieved sv is correlated or not with one or more known variables. Thus, this step gives the user the opportunity to choose the most appropriate number of sv to be used for expression data adjustment \cite{leek2007capturing, jaffe2015practical}.


  \subsubsection{Identification of Surrogate Variables} \label{sv_id}
  Surrogate variables identification, basically, relies on the SVA algorithm by Leek et al. \cite{leek2012sva} \footnote{See \Biocpkg{sva} package}.
  A novel method, that allows the identification of the the maximum number of sv to be used for data adjustment, has been introduced in our package.
  Specifically, it integrates part of the SVA algorithm with custom code. Briefly, we computed eigenvalues of data and then, we calculated the squared of each eigenvalues. The ratio
  between each ``squared eigenvalue'' and the sum of them were then calculated. These values represent a surrogate measure of the ``Fraction of Explained Variance'' (fve) that we would
  obtain by principal component analysis (PCA). Their cumulative sum can be, finally, used to select sv. The method to be applied can be selected in the \Rcode{method} argument of the
  \Rfunction{DaMiR.SV} function. The option \Rcode{"fve"}, \Rcode{"be"} and \Rcode{"leek"} selects, respectively, our implementation or the two methods proposed
  in \Biocpkg{sva} package.
  <<chu_8, dev="pdf">>=
   sv <- DaMiR.SV(data_filt)
  @
  Using default values (\Rcode{"fve"} method and \Rcode{th.fve = 0.95}), we obtained a matrix with 4 sv that is the number of sv which returns ~95\% of variance explained. Figure~\ref{fig_fve} shows all the sv computed by the algorithm with respect to the corresponding fraction of variance explained.

  \begin{figure}[!htbp]
    \includegraphics{figure/chu_8-1}
    \caption{Fraction of Variance Explained. This plot shows the relationship between each sv identified and the corresponding fraction of variance explained. A specific blue dot represents
    the proportion of variance, explained by a sv together with the prior ones. The red dot marks the upper limit of sv that should be used to adjust the data. Here, 4 is the maximum number of sv obtained as it corresponds to $\le$ 95\% of variance explained.}
    \label{fig_fve}
  \end{figure}
  \FloatBarrier


  \subsubsection{Correlation between sv and known covariates} \label{sv_corr}
  Once the sv have been calculated, we may inquire whether these sv capture an unwanted source of variation or may be associated with known variables that the user does not
  wish to correct. For this purpose, we correlate the sv with the known variables stored in the ``data\_filt'' object, in order to decide if all of these sv (or only a subset of them) may be used to
  adjust the data.
  <<chu_9, dev="pdf">>=
   DaMiR.corrplot(sv, colData(data_filt), sig.level = 0.01)
  @
  The \Rfunction{DaMiR.corrplot} function produces a correlation plot where significant correlations (here the threshold is set to
  \Rcode{sig.level = 0.01}) are shown within colored circles (blue or red gradient). In Figure~\ref{fig_corr}, we can see that the first three sv do not significantly correlate
  with any of the used variables and, presumably, recovers the effect of unmeasured covariates. The fourth sv presents, instead, a significant correlation with the ``center'' covariate. The effect of ``center'' might be considered a batch effect and we are interested in adjusting the data for a such confounding variable.\\
  \textbf{Note.} the correlation with ``class'' should always be non significant. In fact, the algorithm for sv identification (embedded into the \Rfunction{DaMiR.SV}
  function) decomposes the expression variation with respect to the variable of interest (i.e. class), that is what we want to preserve by correction \cite{leek2007capturing}.
  Conversely, the user should consider the possibility that hidden factors may present a certain association with the class variable. In this case, we suggest not to remove
  the effect of these sv so that any overcorrection of the expression data is avoided.

  \begin{figure}[!htbp]
    \includegraphics{figure/chu_9-1}
    \caption{Correlation Plot between sv and known covariates. This plot highligths the correlation between sv and known covariates, using both color gradient and circle size. The color ranges from dark red (correlation = -1) to dark blue (correlation = 1) and the circle size is maximum for a correlation equal to 1 or -1 and decreases up to zero. Black crosses help to identify non significant correlations. This plot shows that the first to the third sv do not significantly correlate with any variable, while the fourth is significantly correlated with the ``center'' covariates.}
    \label{fig_corr}
  \end{figure}
  \FloatBarrier


  \subsubsection{Cleaning expression data}
  After sv identification, we need to adjust our expression data. To do this, we exploited the \Rfunction{removeBatchEffect} function of the \Biocpkg{limma} package which is useful for removing unwanted effects from the expression data matrix \cite{ritchie2015limma}. Thus, we adjusted our expression data by setting \Rcode{n.sv = 4} which instructs the algorithm to use the 4 surrogate variables taken from the sv matrix, produced by \Rfunction{DaMiR.SV} function (see Section~\ref{sv_id}).
  <<chu_10, dev="pdf">>=
  data_adjust<-DaMiR.SVadjust(data_filt, sv, n.sv=4)
  assay(data_adjust[c(1:5), c(1:5, 21:25)])
  @
Now, 'data\_adjust' object contains a numeric matrix of log-expression values with sv effects removed.

\subsection{Exploring Data}
Quality Control (QC) is an essential part of any data analysis workflow, because it allows cecking the effects of each action, such as filtering, normalization and data cleaning.
In this context, the function \Rfunction{DaMiR.Allplot} helps to identify how different arguments or specific tasks, such as filtering or normalization, could affect the data. Several diagnostic plots are generated:
\begin{description}[align=left]
  \item [Heatmap] - A distance matrix, based on sample-by-sample correlation, is represented by heatmap and dendrogram using \CRANpkg{pheatmap} package. In addition to 'class', all covariates are shown, using color codes; this helps to simultaneously identify outlier samples and specific clusters, related with class or covariates;

  \item [MultiDimensional Scaling (MDS) plots] - MDS plot, drawn by \CRANpkg{ggplot2} package\cite{wickham2016ggplot2}, provides a visual representation of pattern of proximities (i.e. similarities or distances) among a set of samples, and allows the identification of natural clusters. For the 'class' and for each covariate a MDS plot is drawn.

  \item [Relative Log Expression (RLE) boxplot] - This plot, drawn by \Biocpkg{EDASeq} package\cite{risso2011gc}, helps to visualize the differences between the distributions across samples: medians of each RLE boxplot should be centered around zero and a large shift from zero suggests that samples could have quality problems. Here, different colors means different classes.
\end{description}
In this vignette, \Rfunction{DaMiR.Allplot} is used to appreciate the effect of data adjusting (see Section~\ref{adj_data}). First, we check how data appear just after normalization: the heatmap and RLE plot in Figure~\ref{fig_n1} (upper and lower panel, respectively) and MDS plots in Figures~\ref{fig_n3} and ~\ref{fig_n4} do not highlight the presence of specific clusters.
\textbf{Note.} If a covariate contains missing data (i.e. ``NA'' values), the function cannot draw the plot that bear covariate information. The user is, however, encouraged to impute missing data if s/he considers it meaningful to plot the covariate of interest for ``diagnosis'' purposes.
  <<chu_11, dev="pdf">>=
  # After gene filtering and normalization
  DaMiR.Allplot(data_filt, colData(data_filt))
  @
The \Rcode{df} argument has been supplied using \Rfunction{colData()} function that returns the data frame of covariates stored into the ``data\_filt'' object. Here we used all the variables included into the data frame (i.e. center, sex, age, death and class), although it is possible to use only a subset of them to be plotted.

% Heatmap and RLE
\begin{figure}[!htbp]
\includegraphics{figure/chu_11-1}
\includegraphics{figure/chu_11-7}
\caption{Heatmap and RLE. Heatmap (upper panel): colors in heatmap highlight the distance matrix, obtained by a correlation metric: color gradient ranges from \textit{dark green}, meaning 'minimum distance' (i.e. correlation = 1), to \textit{light green green}. On the top of heatmap, horizontal bars represent class and covariates. Each variable is differently colored (see legend). On the top and on the left side of the heatmap the dendrogram is drawn. Clusters can be easily identified.\\
RLE (lower panel): a boxplot of the distribution of expression values computed as the difference between the expression of each gene and the median expression of that gene accross all samples is shown. Here, since all medians are very close to zero, it seems that all the samples are well-normalized and do not present specific quality problems.
}
\label{fig_n1}
\end{figure}

% MDS center & death
\begin{figure}[!htbp]
  \includegraphics{figure/chu_11-2} %center
  \includegraphics{figure/chu_11-5} %death
  \caption{MultiDimentional Scaling plot. An unsupervised MDS plot is drawn. Samples are colored according to the 'Hardy death scale' (upper panel) and the  'center' covariate (lower panel).}
  \label{fig_n3}
\end{figure}

% MDS sex & class
\begin{figure}[!htbp]
  \includegraphics{figure/chu_11-3} %sex
  \includegraphics{figure/chu_11-6} %class
  \caption{MultiDimentional Scaling plot. An unsupervised MDS plot is drawn. Samples are colored according to 'sex' covariate (upper panel) and 'class' (lower panel).}
  \label{fig_n4}
\end{figure}
\FloatBarrier


\newpage
After removing noise effets from our expression data, as discussed in Section~\ref{adj_data}, we may appreciate the result of sv adjustiment on the data: now, the heatmap in Figure~\ref{fig_n5} and MDS plots in Figures~\ref{fig_n7} and ~\ref{fig_n8} exhibit specific clusters related to \textit{'class'} variable. Moreover, the effect on data distribution is irrelevant: both RLE in Figures~\ref{fig_n1} and ~\ref{fig_n5} show minimal shifts from the zero line.

  <<chu_12, dev="pdf">>=
  # After sample filtering and sv adjusting
  DaMiR.Allplot(data_adjust, colData(data_adjust))
  @


% Heatmap after SV
\begin{figure}[!htbp]
\includegraphics{figure/chu_12-1}
\includegraphics{figure/chu_12-7}
\caption{Heatmap and RLE. Heatmap (upper panel): colors in heatmap highlight the distance matrix, obtained by a correlation metric: color gradient ranges from \textit{dark green}, meaning 'minimum distance' (i.e. correlation = 1), to \textit{light green green}. On the top of heatmap, horizontal bars represent class and covariates. Each variable is differently colored (see legend). The two dendrograms help to quickly identify clusters.\\
RLE (lower panel): Relative Log Expression boxplot. A boxplot of the distribution of expression values computed as the difference between the expression of each gene and the median expression of that gene accross all samples is shown. Here, all medians are very close to zero, meaning that samples are well-normalized.
}
\label{fig_n5}
\end{figure}


% MDS center & death
\begin{figure}[!htbp]
  \includegraphics{figure/chu_12-2} %center
  \includegraphics{figure/chu_12-5} %death
  \caption{MultiDimentional Scaling plot. An unsupervised MDS plot is drawn. Samples are colored according to the 'Hardy death scale' (upper panel) and the  'center' covariate (lower panel).}
  \label{fig_n7}
\end{figure}

% MDS sex & class
\begin{figure}[!htbp]
  \includegraphics{figure/chu_12-3} %sex
  \includegraphics{figure/chu_12-6} %class
  \caption{MultiDimentional Scaling plot. An unsupervised MDS plot is drawn. Samples are colored according to 'sex' covariate (upper panel) and 'class' (lower panel).}
  \label{fig_n8}
\end{figure}


\FloatBarrier
\newpage
\subsection{Feature Selection}
The previous step(s) returned an adjusted expression matrix with the effect of sv removed. However, the number of features in the dataset is still high and greatly exceeds the number of observations. We have to deal, here, with the well-known principle that occurs with high-dimensional data known as the ``curse of dimensionality''. Adding noise features that are not truly associated with the response (i.e. class) may lead, in fact, to a worsening model accuracy. In this situation, the user needs to remove those features that bear irrelevant or redundant information. The feature selection technique implemented here does not alter the original representation of the variables, but simply selects a subset of them. It includes three different steps briefly described in the following paragraphs.

\subsubsection{Variable selection in Partial Least Squares (PLS)}
The first step allows the user to exclude all non-informative class-related features using a backward variable elimination procedure  \cite{mehmood2012review}. The \Rfunction{DaMiR.FSelect} function embeds a principal component analysis (PCA) to identify principal components (PCs) which correlate with ``class''. The correlation is defined by the user through the \Rcode{th.corr} argument. The higher the correlation, the lower the number of PCs returned. Importantly, users should pay attention to appropriately set the \Rcode{th.corr} argument since the total number of retrieved features depends, indeed, on the number of the selected PCs. \\
The number of class-correlated PCs is then internally used by the function to perform a backward variable elimination PLS and remove those variables that are less informative with respect to class \cite{frank1987intermediate}.\\
\textbf{Note.} Before running the \Rfunction{DaMiR.FSelect} function, we need to transpose our normalized expression data. Actually, the function \Rfunction{DaMiR.transpose} transposes data but also tries to prevent the use of tricky feature labels. The ``-'' and ``.'' characters within variable labels (commonly found, for example, in gene symbols) may, in fact, cause errors if included in the model design as it is required to execute part of the code of the \Rfunction{DaMiR.FSelect} function. Thus, we, firstly, search and, eventually, replace them with non causing error characters.
 <<chu_13>>=
 data_clean<-DaMiR.transpose(assay(data_adjust))
 df<-colData(data_adjust)
 data_reduced <- DaMiR.FSelect(data_clean, df, th.corr=0.4)
 @
The data\_reduced object returns an expression matrix with important features. As we can see, the initial number of 19052 features has been reduced to 286.

\subsubsection{Removing highly correlated features}
Some of the returned important features may, however, be highly correlated. To prevent the inclusion of redundant features that may decrease the model performance during the classification step, we apply a function that produces a pair-wise absolute correlation matrix. When two features present a correlation higher than \Rcode{th.corr} argument, the algorithm calculates the mean absolute correlation of each feature and, then, removes the feature with the largest mean absolute correlation.
 <<chu_14, dev="pdf">>=
 data_reduced <- DaMiR.FReduct(data_reduced$data)
 DaMiR.MDSplot(data_reduced, df)
  @
In our example, we used a Spearman's correlation metric and a correletion threshold of 0.85 (default). This further reduction step filters out 79 highly correlated genes over the 286 returned by the \Rfunction{DaMiR.FSelect}. The figure below shows the MDS plot drawn by the use of the expression matrix of the remaining 207 genes.
\begin{figure}[!htbp]
\includegraphics{figure/chu_14-1}
\caption{
MultiDimentional Scaling plot. A MDS plot is drawn, considering only most informative genes, obtained after feature selection: color code is referred to 'class'.
}
\label{fig_MDS}
\end{figure}
\FloatBarrier
\subsubsection{Ranking and selecting important features}
The above functions produced a reduced matrix of variables. The number of reduced variables might be, however, too high to provide faster and cost-effective classification models. Accordingly, we should properly select a subset of the most informative features. The \Rfunction{DaMiR.FSort} function implements a procedure in order to rank features by their importance. The method implements a multivariate filter technique (i.e. \textit{RReliefF}) that assessess the relevance of features by looking at the properties of the data (for details see the \Rfunction{relief} function of the \Biocpkg{FSelector} package) \cite{kononenko1994estimating, robnik1997adaptation}. The function produced a data frame with two columns, which stores features ranked by importance scores: a \textit{RReliefF} score and \textit{scaled.RReliefF} value; the latter is computed implementing a ``z-score'' standardization procedure on \textit{RReliefF} values.\\
\textbf{Note.} This step may be time-consuming if a data matrix with a high number of features is inputted. We observed, in fact, that there is a quadratic relationship between execution time of the algorithm and the number of features. The user is advised with a message about the estimated time needed to compute the score and rank the features. Reasonably, we strongly suggest to filter out non informative features by the \Rfunction{DaMiR.FSelect} and \Rfunction{DaMiR.FReduct} functions before performing this step.
<<chu_15, dev="pdf">>=
  # Rank genes by importance:
  df.importance <- DaMiR.FSort(data_reduced, df)
  head(df.importance)
@
After the importance score is calculated, a subset of features can be selected and used as predictors for the classification.
The function \Rfunction{DaMiR.FBest} is used to select a small subset of predictors:
<<chu_16, dev="pdf">>=
  # Select Best Predictors:
  selected_features <- DaMiR.FBest(data_reduced, ranking=df.importance,
                                   n.pred = 5)
  selected_features$predictors
  # Dendrogram and heatmap:
  DaMiR.Clustplot(selected_features$data, df)
  @
Here, we selected the first 5 genes (default) ranked by importance. \\
\textbf{Note.} The user may also wish to select ``automatically'' (i.e. not defined by the user) the number of important genes. This is possible by setting \Rcode{autoselect="yes"} and a threshold for the \textit{scaled.RReliefF}, i.e. \Rcode{th.zscore} argument.
These normalized values (rescaled to have a mean of 0 and standard deviation of 1) make it possible to compare predictors ranking obtained by running the pipeline with different parameters.

\begin{figure}[!htbp]
\includegraphics{figure/chu_15-1}
\caption{
Feature Importance Plot. The dotchart shows the list of top 50 genes, sorted by RReliefF importance score. This plot may be used to select the most important predictors that are then used for classification.
}
\label{fig_impo}
\end{figure}

\begin{figure}[!htbp]
\includegraphics{figure/chu_16-1}
\caption{
Clustergram. The clustergram is generated by using the expression values of the 5 predictors selected by \Rfunction{DaMiR.FBest} function. As for the heatmap generated by \Rfunction{DaMiR.Allplot}, 'class' and covariates are drawn as horizontal and color coded bars.
}
\label{fig_n9}
\end{figure}
\FloatBarrier


\subsection{Classification}
All the steps executed so far, allowed to normalize, clean and reduce the original expression matrix; the objective was to get a subset of original data as informative as possible, in order to carry out a classification analysis. In this paragraph, we describe the statistical learning strategy we implemented to tackle binary classification problems.\\
A meta-learner is built, combining the outputs of 6 different classifiers through a ``Stacking'' strategy. Currently, there is no a gold standard for creating the best rule to combine predictions \cite{polikar2006ensemble}. We decided to implement a framework that relies on the ``weighted majority voting'' approach \cite{LITTLESTONE1994212}. In particular, our method estimates a weight for each classifier, based on its own accuracy, and then use these weights, together with predictions, to tune a decision rule (i.e. meta-learner); briefly, first a training set (TR1) and a test set (TS1) are generated by ``Bootstrap'' sampling. Then, sampling again from subset TR1, another pair of training (TR2) and test set (TS2) were obtained. TR2 is used to train RF, NB, SVM, 3kNN, LDA and LR classifiers, whereas TS2 is used to test their accuracy and to calculate weights ($w$) by formula:

\begin{equation}\label{ens_weight1}
  w_{classifier_{i}} = \frac{Accuracy_{classifier_{i}}}{\displaystyle\sum_{j=1}^{N} Accuracy_{classifier_{j}}}
\end{equation}
where $i$ is a specific classifiers and $N$ is the total number of them (here, $N = 6$). Using this approach:

\begin{equation}\label{ens_weight2}
  \displaystyle\sum_{i=1}^{N} w_{i} = 1
\end{equation}

The higher the value of $w_i$, the more accurate is the classifier.\\
The performance of the meta-learner (labelled as ``Ensemble'') is evaluated by using TS1. The decision rule of the meta-learner is made by a linear combination of the products between weigths ($w$) and binary (0 or 1) predictions ($Pr$) of each classifier; for each sample \textit{k}, the prediction is computed by:
\begin{equation}\label{ens_learn}
  \begin{split}
    Pr_{(k, Ensemble)} = w_{RF} * Pr_{(k, RF)} + w_{NB} * Pr_{(k, NB)} + w_{SVM} * Pr_{(k, SVM)} + \\
    + w_{3kNN} * Pr_{(k, 3kNN)} + w_{LDA} * Pr_{(k, LDA)} + w_{LR} * Pr_{(k, LR)}
  \end{split}
\end{equation}

$Pr_{(k, Ensemble)}$ ranges from 0 (high probability to belong to one class) to 1 (high probability to belong to the other class); predictions close to 0.5 have to be considered as made by chance. This process is repeated several times to assess the robustness of the set of predictors used.\\

The procedure, described above, is implemented in the \Rfunction{DaMiR.EnsembleLearning} function, where \Rcode{fSample.tr}, \Rcode{fSample.tr.w} and \Rcode{iter} arguments allow to algorithm tuning.\\
In this case, before implementing the classification procedure, we created a ``class'' vector. To speed up the execution time of the function, we set \Rcode{iter = 30} (default is 100) even though we suggest to use an higher number of iterations to obtain more accurate results.

<<chu_17, dev="pdf">>=

  Classification_res <- DaMiR.EnsembleLearning(selected_features$data,
                          classes=df$class, fSample.tr = 0.5,
                          fSample.tr.w = 0.5, iter = 30)
@
The function returns a list containing: a matrix of accuracies of each classifier in each iteration, a matrix of weights used for each classifier in each iteration and a list of all models generated in each iteration. These objects can be accessed using the \$ accessor.

\begin{figure}[!htbp]
\includegraphics{figure/chu_17-1}
\caption{
Accuracies Comparison. The violin plot highlights the classification accuracy of each classifier, computed at each iteration; a black dot represents a specific accuracy value while the shape of each ``violin'' is drawn by a Gaussian kernel density estimation. Averaged accuracies and standard deviations are represented by white dots and lines.
}
\label{fig_n10}
\end{figure}
\FloatBarrier

As shown in Figure~\ref{fig_n10}, almost all the weak classifiers show a very high (greater than 95\% in average) classification accuracy (RF: $96.17 \pm 3.13$, SVM: $97.33 \pm 2.54$, NB: $97 \pm 3.11$, LDA: $94.83 \pm 6.36$, LR: $91.5 \pm 7.78$, 3kNN: $95 \pm 4.73$). As discussed in Section~\ref{intro}, the meta-learner is more influenced by better weak classifiers than inferior ones, which ensures ``Ensemble'' to reach a classification accuracy equal to $97 \pm 2.49$.

\subsection{Exporting output data}
\damir{} has been designed to allow users to export the outputs of each function, which consist substantially in \Robject{matrix} or \Robject{data frame} objects. The export can be done, using the base R functions, such as \Rfunction{write.table} or \Rfunction{write.csv}. For example, we could be interested in saving normalized data matrix, stored in ``data\_norm'' in a tab-delimited file:

<<chu_export, dev="pdf", eval=FALSE>>=
outputfile <- "DataNormalized.txt"
write.table(data_norm, file = outputfile_norm, quote = FALSE, sep = "\t")
@


\section{Standard workflow and variations}
In this section, we highlight and discuss how variations to the \damir{} pipeline could affect the final classification results. Results will be discussed in Section~\ref{perf_comp}. \\
\textbf{Note.} For simplicity, here we do not produce all plots, except for the violin plot generated by \Rfunction{DaMiR.EnsembleLearning}, used for performances comparison. However, the usage of \Rfunction{DaMiR.Allplot}, \Rfunction{DaMiR.corrplot}, \Rfunction{DaMiR.Clustplot} and \Rfunction{DaMiR.MDSplot} \textbf{is crucial} to check the effect of each process.


\subsection{Without cleaning the data} \label{not_adj}
Adjusting the data step, described in Section~\ref{adj_data}, could be skipped if we assume that data are not affected by any batches (known or unknown), or we do not want to take them into account. In this case, VST or rlog normalized data will be used for the following feature selection step without any other adjustment. However, we sincerely suggest to always check the presence of any confounding factors since their effects could dramatically affect the results.
We set the same seed used in Section~\ref{sect2} (\Rcode{set.seed(12)}), which ensures the right comparisons between results.

<<chu_18, dev="pdf", echo=FALSE>>=
## Feature Selection
data_clean_2<-DaMiR.transpose(assay(data_filt))
df_2<-colData(data_filt)
data_reduced_2 <- DaMiR.FSelect(data_clean_2, df_2, th.corr=0.4)
# data_reduced$data[c(1:5), c(1:5,21:25)]
data_reduced_2 <- DaMiR.FReduct(data_reduced_2$data)
df.importance_2 <- DaMiR.FSort(data_reduced_2, df_2)
head(df.importance_2)
selected_features_2 <- DaMiR.FBest(data_reduced_2, ranking=df.importance_2, n.pred=5)
selected_features_2$predictors

## Classification
Classification_res_2 <- DaMiR.EnsembleLearning(selected_features_2$data,
                                             classes=df_2$class, fSample.tr = 0.5, fSample.tr.w = 0.5, iter = 30)


@


\subsection{Changing the number of predictors} \label{more_pred}
The number of predictors obviously affects the classification accuracy: a small number of features could not be sufficient to ensure a good classification power; on the other hand, many predictors allow better classification accuracies if they are truly associated with the response, but at the cost of a possible reduction of the generalization ability. In this case, we decide to use 10 predictors instead of the 5 employed in the standard workflow.

<<chu_19, dev="pdf", echo=FALSE>>=

selected_features_3 <- DaMiR.FBest(data_reduced, ranking=df.importance, n.pred = 10)
selected_features_3$predictors

## Classification
Classification_res_3 <- DaMiR.EnsembleLearning(selected_features_3$data,
                                             classes=df$class, fSample.tr = 0.5, fSample.tr.w = 0.5, iter = 30)

@

\subsection{Adding heterogeneous features for classification} \label{add_feat}
Classification algorithms are ables to work with heterogeneous features, i.e. coming from different sources, such as other -omic data techniques, environment, demography etc. Both factorial and continuous variables may be used. \\
In this example, we used the data of the ``sex'' and ``age'' covariates stored in the second and third column of the ``df'' data frame.

<<chu_20, dev="pdf", echo=FALSE>>=
## Classification
Classification_res_4 <- DaMiR.EnsembleLearning(selected_features$data,
                                             classes=df$class, covariates = df[,2:3], fSample.tr = 0.5, fSample.tr.w = 0.5, iter = 30)

@

\subsection{Standard workflow, changing the seed} \label{std_seed}
The aim is to check the robustness of ``Ensemble'', changing training and test sets used before.\\
\textbf{Note.} \damir{} has also a helper function, called \Rfunction{DaMiR.goldenDice}, which automatically generates a number, combining date and time information; the idea is to be as random as possible for training and test subsets generation, during classification step.
Here, we changed the seed (\Rfunction{set.seed(12345)}) before the classification procedure that will slightly affect the accuracy results.

<<chu_21, dev="pdf", echo=FALSE>>=
# change the seed
set.seed(12345)
## Classification
Classification_res_5 <- DaMiR.EnsembleLearning(selected_features$data,
                                             classes=df$class, fSample.tr = 0.5, fSample.tr.w = 0.5, iter = 30)

@


\subsection{Performances comparison} \label{perf_comp}
In this paragraph, we discuss the effect of each variation of the standard workflow, comparing performances of each ``Ensemble'' meta-learner.

Figure~\ref{fig_n11} condenses results of \damir{}'s standard workflow variations, proposed in this Section. Results will be discussed separately, in comparison with ``Standard Workflow'',taken as the reference, described in Section~\ref{sect2} and depicted in Figure~\ref{fig_n10}.

\begin{figure}[!htbp]
\includegraphics[width=.6\textwidth]{figure/chu_18-2} % no SV
\includegraphics[width=.6\textwidth]{figure/chu_19-1} % 10 predictors
\includegraphics[width=.6\textwidth]{figure/chu_20-1} % adding SEx
\includegraphics[width=.6\textwidth]{figure/chu_21-1} % Standard WF changing seed
\caption{Performance comparison. Each violin plot shows the effect of a specific modification to \damir{} standard workflow, described in Section~\ref{sect2}; in particular, we can see at the top left part of this figure, results of Section~\ref{std_seed} workflow (\textit{Without cleaning the data}); at the top right, results of Section~\ref{not_adj}'s workflow (\textit{Changing the number of predictors}); at the bottom left, results of Section~\ref{more_pred} workflow (\textit{Adding heterogeneous features for classification}); and, at the bottom right, results of Section~\ref{add_feat} workflow (\textit{Standard Workflow, changing seed})}.
\label{fig_n11}
\end{figure}


\begin{description}[align=left]

\item ['Standard Workflow' vs 'Without cleaning the data'.] Without adjusting data (following the steps described in Section~\ref{adj_data}), performances usually decrease; this could be explained by the fact that some noise, probably coming from unknown source of variation, is present in the dataset. In this example, overall accuracies drastically decrease below 90\% for all classifiers. See Figure~\ref{fig_n10} and upper right panel in Figure~\ref{fig_n11}.

\item ['Standard Workflow' vs 'Changing the number of predictors'.] It is always good practice to achieve a trade-off between model complexity, generalization capability and classification power. Increasing the number of predictors changes the performances, even if in this case the classification accuracy does not increase or decrease uniformly among the classifiers. See Figure~\ref{fig_n10} and lower left panel in Figure~\ref{fig_n11}.

\item ['Standard Workflow' vs 'Adding heterogeneous features for classification'.] We included the ``sex'' and ``age'' variables together with predictor genes to generate new classification models. Demographic variables may play an important role in some classification settings and may, in fact, impact prediction. However, in this case, these two variables do not impact deeply on prediction of the classifiers, with the exception of LDA and 3kNN that, conversely, showed a marked accuracy decrease. However, the poor performance of these two classifier (that are, in fact, less suitable to fit categorical variables) did not affected the overall performance of the ``Ensemble'' learner. See Figure~\ref{fig_n10} and bottom lower panel in Figure~\ref{fig_n11}.

\item ['Standard Workflow' vs 'Standard Workflow, changing the seed'.] It is well known that, changing training and test sets, a specific classifier may present different performances. However, the stability and the robustness of a classifier affects the generalization capability. An advantage of adopting a Staking strategy is to be as stable and robust as possible. In this case the accuracy of ``Ensemble'' meta-learner is quite similar despite the use of different sampling, even though the performances of weak classifiers display some differences. This means that the choice of a single classifier, even when it shows the higher prediction accuracy, do not always ensure obtaining the best accuracy on new unseen data. The ``Ensemble'', instead, is always close to the best prediction accuracy. See Figure~\ref{fig_n10} and upper left panel in Figure~\ref{fig_n11}.
\end{description}


\FloatBarrier


\section{Session Info}
<<sessInfo, results="asis", echo=FALSE>>=
toLatex(sessionInfo())
@

\bibliography{library}

\end{document}





